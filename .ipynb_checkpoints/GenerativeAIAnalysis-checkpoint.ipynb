{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e25e0af-72a9-436a-a993-6b5aec3ebd04",
   "metadata": {},
   "source": [
    "### Scenario 1: Retail Inventory Analysis\n",
    "<b>Scenario:</b> You're working at a retail chain analyzing inventory turnover. The store manager needs to understand stock movement patterns to optimize ordering.\n",
    "\n",
    "<b>Data Structure: DataFrame\n",
    "\n",
    "- product_id\n",
    "- category\n",
    "- stock_level\n",
    "- last_restock_date\n",
    "- sales_last_30_days\n",
    "- supplier_lead_time\n",
    "- unit_cost\n",
    "\n",
    "<b>Task: Generate code to:\n",
    "\n",
    "- Calculate inventory turnover rates.\n",
    "- Identify slow-moving items.\n",
    "- Predict potential stockouts.\n",
    "- Create visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bc749e-befc-4170-88cd-6f09b296c0c1",
   "metadata": {},
   "source": [
    "### Answer: \n",
    "\"I want to analyze a retail chain inventory turnover. I need to focus on the inventory level with the stock movement patterns and ordering optimization. Provide me the guidelines for my analysis: \n",
    "1. Data Validation\n",
    "2. Inventory turnover rates \n",
    "3. Stock movements analysis\n",
    "\n",
    "Please provide some key aspects and considerations.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb4db01-dd51-44f3-bd4b-01e9af70b0e3",
   "metadata": {},
   "source": [
    "\"Using the pandas DataFrame, the columns associated with my data structures are: product_id, category,  stock_level, last_stock_date, sales_last_30_days, supplier_lead_time & unit_cost. Help me write a Python code to load and validate this data for my analysis to calculate inventory turnover rates, identify slow-moving items and predict potential stockouts.\"\n",
    "                                                                                                                                                                                                                                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45fa9122-ebdd-4524-81f7-f59614adb452",
   "metadata": {},
   "source": [
    "\"Can you provide me the function to generate the report to display the slow movers and stockout prediction with error handling.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2330f970-39ad-4366-9ce1-52e073f494b3",
   "metadata": {},
   "source": [
    "\"Use Seaborn and matplotlib to create python visualization code to show appropriate chart and make sure to include clear labels and titles. We can use the line chart for the inventory turnover rates and bar chart for the slow-moving items and appropriate chart for the potential stockout items.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3638e5bb-85cc-4a58-ad01-f5b41974f752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_inventory_report(df):\n",
    "    \"\"\"\n",
    "    Generate a report to identify slow-moving inventory and potential stockouts.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Inventory data with columns:\n",
    "            - product_id\n",
    "            - category\n",
    "            - stock_level\n",
    "            - last_stock_date\n",
    "            - sales_last_30_days\n",
    "            - supplier_lead_time\n",
    "            - unit_cost\n",
    "            \n",
    "    Returns:\n",
    "        slow_movers (pd.DataFrame): Products with low sales and high DSI.\n",
    "        stockout_risks (pd.DataFrame): Products predicted to stock out soon.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure necessary columns exist\n",
    "        required_cols = [\n",
    "            'product_id', 'category', 'stock_level', 'last_stock_date',\n",
    "            'sales_last_30_days', 'supplier_lead_time', 'unit_cost'\n",
    "        ]\n",
    "        for col in required_cols:\n",
    "            if col not in df.columns:\n",
    "                raise ValueError(f\"Missing required column: {col}\")\n",
    "\n",
    "        # Convert dates\n",
    "        df['last_stock_date'] = pd.to_datetime(df['last_stock_date'], errors='coerce')\n",
    "\n",
    "        # Drop invalid or missing values\n",
    "        df = df.dropna(subset=required_cols)\n",
    "\n",
    "        # Filter out invalid values\n",
    "        numeric_cols = ['stock_level', 'sales_last_30_days', 'supplier_lead_time', 'unit_cost']\n",
    "        for col in numeric_cols:\n",
    "            df = df[df[col] >= 0]\n",
    "\n",
    "        # Calculate COGS and average inventory value\n",
    "        df['COGS_last_30_days'] = df['sales_last_30_days'] * df['unit_cost']\n",
    "        df['avg_inventory_cost'] = df['stock_level'] * df['unit_cost']\n",
    "\n",
    "        # Prevent division by zero using np.where\n",
    "        df['inventory_turnover'] = np.where(\n",
    "            df['avg_inventory_cost'] > 0,\n",
    "            df['COGS_last_30_days'] / df['avg_inventory_cost'],\n",
    "            0\n",
    "        )\n",
    "        df['DSI'] = np.where(\n",
    "            df['COGS_last_30_days'] > 0,\n",
    "            df['avg_inventory_cost'] / df['COGS_last_30_days'] * 30,\n",
    "            np.inf\n",
    "        )\n",
    "\n",
    "        # Identify slow movers: low sales + high DSI\n",
    "        slow_movers = df[(df['sales_last_30_days'] <= 5) & (df['DSI'] > 30)].copy()\n",
    "        slow_movers = slow_movers[['product_id', 'category', 'sales_last_30_days', 'DSI']]\n",
    "\n",
    "        # Predict stockout risks\n",
    "        df['daily_sales_rate'] = df['sales_last_30_days'] / 30\n",
    "        df['days_until_stockout'] = np.where(\n",
    "            df['daily_sales_rate'] > 0,\n",
    "            df['stock_level'] / df['daily_sales_rate'],\n",
    "            np.inf\n",
    "        )\n",
    "\n",
    "        stockout_risks = df[df['days_until_stockout'] <= df['supplier_lead_time']].copy()\n",
    "        stockout_risks = stockout_risks[[\n",
    "            'product_id', 'category', 'stock_level', 'days_until_stockout', 'supplier_lead_time'\n",
    "        ]]\n",
    "\n",
    "        return slow_movers.reset_index(drop=True), stockout_risks.reset_index(drop=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[Error] Failed to generate inventory report: {e}\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93082714-b124-4506-ab75-4aff51d720c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "df = pd.read_csv(\"inventory_data.csv\")\n",
    "\n",
    "# Generate the report\n",
    "slow_movers_df, stockout_risks_df = generate_inventory_report(df)\n",
    "\n",
    "# Display results\n",
    "print(\"SLOW MOVING ITEMS:\")\n",
    "print(slow_movers_df)\n",
    "\n",
    "print(\"\\nSTOCKOUT RISK ITEMS:\")\n",
    "print(stockout_risks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "065e2620-3db5-439c-a733-621b84644e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_inventory_metrics(df, slow_movers, stockout_risks):\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # 1. Line Chart: Inventory Turnover by Product ID\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    turnover_data = df[['product_id', 'inventory_turnover']].sort_values(by='inventory_turnover')\n",
    "    sns.lineplot(data=turnover_data, x='product_id', y='inventory_turnover', marker='o')\n",
    "    plt.title(\"Inventory Turnover Rate by Product\")\n",
    "    plt.xlabel(\"Product ID\")\n",
    "    plt.ylabel(\"Inventory Turnover Rate\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Bar Chart: Slow Moving Items by DSI\n",
    "    if not slow_movers.empty:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        slow_movers_sorted = slow_movers.sort_values(by='DSI', ascending=False)\n",
    "        sns.barplot(data=slow_movers_sorted, x='product_id', y='DSI', hue='category')\n",
    "        plt.title(\"Slow-Moving Items (High DSI > 30)\")\n",
    "        plt.xlabel(\"Product ID\")\n",
    "        plt.ylabel(\"Days Sales of Inventory (DSI)\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.legend(title='Category')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No slow-moving items to plot.\")\n",
    "\n",
    "    # 3. Scatter Plot: Potential Stockout Risk\n",
    "    if not stockout_risks.empty:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.scatterplot(\n",
    "            data=stockout_risks,\n",
    "            x='supplier_lead_time',\n",
    "            y='days_until_stockout',\n",
    "            hue='category',\n",
    "            size='stock_level',\n",
    "            sizes=(40, 200),\n",
    "            alpha=0.7\n",
    "        )\n",
    "        plt.axhline(y=0, color='gray', linestyle='--')\n",
    "        plt.plot([0, max(stockout_risks['supplier_lead_time'])],\n",
    "                 [0, max(stockout_risks['supplier_lead_time'])],\n",
    "                 color='red', linestyle='--', label='Stockout Threshold')\n",
    "        plt.title(\"Stockout Risk: Days Until Stockout vs Supplier Lead Time\")\n",
    "        plt.xlabel(\"Supplier Lead Time (Days)\")\n",
    "        plt.ylabel(\"Estimated Days Until Stockout\")\n",
    "        plt.legend(title='Category')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No stockout risk items to plot.\")\n",
    "\n",
    "# Visualize\n",
    "visualize_inventory_metrics(df, slow_movers_df, stockout_risks_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a35da8f-efc6-44ef-9383-3f3dc1bce94d",
   "metadata": {},
   "source": [
    "### Discussion Questions to Answer:\n",
    "\n",
    "1. How did different prompts handle date calculations?\n",
    "\n",
    "Different prompts handled date calculations by:\n",
    "- Parsing dates: Converting last_stock_date to datetime format using pd.to_datetime() with error handling.\n",
    "\n",
    "- Stockout prediction: Used historical sales (sales_last_30_days) to estimate daily sales rate, then calculated days_until_stockout by dividing stock_level by the daily rate.\n",
    "\n",
    "- Avoiding errors: Used np.where() to handle division-by-zero cases (e.g., when sales are zero) to prevent infinite or undefined values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2ccdd1-265c-476d-bc82-ca536b982672",
   "metadata": {},
   "source": [
    "2. What visualization approaches were suggested?\n",
    "\n",
    "The suggested visualization approaches included:\n",
    "\n",
    "- Line chart for inventory turnover rates by product\n",
    "  \n",
    "- Bar chart to highlight slow-moving items based on DSI\n",
    "\n",
    "  \n",
    "- Scatter plot to visualize stockout risk (days until stockout vs. supplier lead time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc8ba51-a615-4c2a-b4e7-af680fe58124",
   "metadata": {},
   "source": [
    "3. How was error handling implemented?\n",
    "   \n",
    "Error handling was implemented using:\n",
    "\n",
    "- A try-except block to catch and report issues during report generation\n",
    "\n",
    "- Column presence checks with custom error messages\n",
    "\n",
    "- Data cleaning steps to drop missing or invalid entries\n",
    "\n",
    "- Use of np.where() to prevent division-by-zero in metric calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621cd307-2bbf-4c29-96df-b8a485b04327",
   "metadata": {},
   "source": [
    "### Scenario 2: Website Analytics Debug\n",
    "<b>Scenario:</b> The marketing team reports that the user engagement metrics code is showing impossible results (bounce rates over 100%, negative session times).\n",
    "\n",
    "<b>Problematic Code:\n",
    "\n",
    "    def analyze_user_engagement(logs_df):\n",
    "        metrics = {\n",
    "            'bounce_rate': logs_df.groupby('session_id')['page_views'].apply(\n",
    "                lambda x: x == 1).mean(),\n",
    "            'avg_session_time': logs_df.groupby('session_id')['duration'].sum(),\n",
    "            'pages_per_session': logs_df.groupby('session_id')['page_views'].mean()\n",
    "        }\n",
    "    \n",
    "    device_metrics = logs_df.groupby('device_type').agg({\n",
    "        'session_id': 'count',\n",
    "        'duration': 'mean',\n",
    "        'page_views': 'sum'\n",
    "    })\n",
    "    \n",
    "    return metrics, device_metrics\n",
    "<b>Task:\n",
    "\n",
    "Debug the calculations.\n",
    "- Add data validation.\n",
    "- Implement proper time calculations.\n",
    "- Create summary visualizations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44647300-05bb-49e1-ba66-16cbe56b6efa",
   "metadata": {},
   "source": [
    "Answer: \n",
    "\n",
    "\"Can you help me fix a problematic code. The user engagement metrics code is showing impossible results (bounce rates over 100%, negative session times). Can you also provide a short explanation of all the fixes applied to the original code and handle the errors appropriately. The code is given below: \n",
    "\n",
    "    def analyze_user_engagement(logs_df):\n",
    "        metrics = {\n",
    "            'bounce_rate': logs_df.groupby('session_id')['page_views'].apply(\n",
    "                lambda x: x == 1).mean(),\n",
    "            'avg_session_time': logs_df.groupby('session_id')['duration'].sum(),\n",
    "            'pages_per_session': logs_df.groupby('session_id')['page_views'].mean()\n",
    "        }\n",
    "    \n",
    "    device_metrics = logs_df.groupby('device_type').agg({\n",
    "        'session_id': 'count',\n",
    "        'duration': 'mean',\n",
    "        'page_views': 'sum'\n",
    "    })\n",
    "    \n",
    "    return metrics, device_metrics\n",
    "\n",
    "\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e2cf26-6c8e-45a2-91e7-7648ad22d51d",
   "metadata": {},
   "source": [
    "\"Can you write a code to include the visualization for user engagement metric with bar chart and histograms and include proper error handling.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7724c4-2ae3-4de3-8e90-ea0d55da1e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_user_engagement(logs_df):\n",
    "    import pandas as pd\n",
    "\n",
    "    try:\n",
    "        # Ensure necessary columns exist\n",
    "        required_cols = {'session_id', 'duration', 'page_views', 'device_type'}\n",
    "        if not required_cols.issubset(logs_df.columns):\n",
    "            raise ValueError(f\"Missing required columns: {required_cols - set(logs_df.columns)}\")\n",
    "\n",
    "        # Remove negative durations and page_views (data cleaning)\n",
    "        logs_df = logs_df[(logs_df['duration'] >= 0) & (logs_df['page_views'] >= 0)]\n",
    "\n",
    "        # Group by session for session-level metrics\n",
    "        session_data = logs_df.groupby('session_id').agg({\n",
    "            'duration': 'sum',\n",
    "            'page_views': 'sum'\n",
    "        })\n",
    "\n",
    "        # Bounce rate: percentage of sessions with exactly 1 page view\n",
    "        bounce_rate = (session_data['page_views'] == 1).mean() * 100\n",
    "\n",
    "        # Average session time\n",
    "        avg_session_time = session_data['duration'].mean()\n",
    "\n",
    "        # Pages per session\n",
    "        pages_per_session = session_data['page_views'].mean()\n",
    "\n",
    "        metrics = {\n",
    "            'bounce_rate': round(bounce_rate, 2),\n",
    "            'avg_session_time': round(avg_session_time, 2),\n",
    "            'pages_per_session': round(pages_per_session, 2)\n",
    "        }\n",
    "\n",
    "        # Device-level metrics\n",
    "        device_metrics = logs_df.groupby('device_type').agg(\n",
    "            sessions=('session_id', pd.Series.nunique),\n",
    "            avg_duration=('duration', 'mean'),\n",
    "            total_page_views=('page_views', 'sum')\n",
    "        ).round(2)\n",
    "\n",
    "        return metrics, device_metrics\n",
    "\n",
    "    except Exception as e:\n",
    "        return {'error': str(e)}, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc691414-84d1-4914-94d5-5fbcc963bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_bounce_rate(bounce_rate):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.bar(['Bounce Rate'], [bounce_rate], color='orange')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title('Bounce Rate')\n",
    "    plt.text(0, bounce_rate + 2, f\"{bounce_rate:.1f}%\", ha='center')\n",
    "    plt.show()\n",
    "\n",
    "def plot_session_duration_distribution(logs_df):\n",
    "    session_durations = logs_df.groupby('session_id')['duration'].sum()\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.histplot(session_durations, bins=30, kde=True)\n",
    "    plt.xlabel('Session Duration (seconds)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Session Durations')\n",
    "    plt.show()\n",
    "\n",
    "def plot_device_metrics(device_metrics):\n",
    "    device_metrics.plot(kind='bar', figsize=(10, 6))\n",
    "    plt.title('Engagement Metrics by Device Type')\n",
    "    plt.ylabel('Values')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "metrics, device_metrics = analyze_user_engagement(logs_df)\n",
    "\n",
    "if 'error' not in metrics:\n",
    "    plot_bounce_rate(metrics['bounce_rate'])\n",
    "    plot_session_duration_distribution(logs_df)\n",
    "    plot_device_metrics(device_metrics)\n",
    "else:\n",
    "    print(\"Error:\", metrics['error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dc1485-cd8c-442f-9752-62f520b34e37",
   "metadata": {},
   "source": [
    "### Discussion Questions:\n",
    "\n",
    "1. How did different prompts approach error identification?\n",
    "\n",
    "Different prompts approached error identification by first validating the input data for completeness and correctness (e.g., checking for required columns), then filtering out invalid or negative values, and finally using try-except blocks to catch unexpected errors during processing. This layered approach ensures both data quality and robust error handling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d045e551-c220-4c90-9f4f-1633389793fd",
   "metadata": {},
   "source": [
    "2. What validation methods were suggested?\n",
    "\n",
    "The suggested validation methods included checking for the presence of required columns, filtering out invalid or negative values, and verifying data types to ensure data integrity before analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eeaf6b-6664-42d4-842d-491817311257",
   "metadata": {},
   "source": [
    "3. How was time handling improved?\n",
    "\n",
    "Time handling was improved by aggregating session durations correctly—summing durations per session and then calculating the average session time while filtering out negative or invalid duration values to ensure realistic and accurate metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b40fea3-6a84-4427-9693-009fc0a8bbce",
   "metadata": {},
   "source": [
    "### Scenario 3: Customer Segmentation Query\n",
    "<b>Scenario:</b> The product team needs to segment customers based on their purchasing behavior for a new feature rollout.\n",
    "\n",
    "<b>Database Schema:\n",
    "\n",
    "- user_activity\n",
    "    - user_id\n",
    "    - last_login_date\n",
    "    - feature_usage_count\n",
    "    - account_type\n",
    "- transactions\n",
    "    - transaction_id\n",
    "    - user_id\n",
    "    - transaction_date\n",
    "    - amount\n",
    "    - platform\n",
    "- user_preferences\n",
    "    - user_id\n",
    "    - communication_preference\n",
    "    - interface_theme\n",
    "    - notification_settings\n",
    "      \n",
    "<b>Task: Create a SQL query to identify:\n",
    "\n",
    "- Active users (logged in last 30 days)\n",
    "- Filter by high-value customers (top 20% by spending)\n",
    "- User preference trends for the identified customers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458dadf1-ca6a-4576-8848-5b0d984019c8",
   "metadata": {},
   "source": [
    "Answer: \n",
    "\n",
    "\"I am working with SQL query where I need to segment customers based on their purchasing behavior for a new feature rollout. Can you write me a SQL code based on the following database schema and scenarios: \n",
    "\n",
    "<b>Database Schema:\n",
    "\n",
    "- user_activity\n",
    "    - user_id\n",
    "    - last_login_date\n",
    "    - feature_usage_count\n",
    "    - account_type\n",
    "- transactions\n",
    "    - transaction_id\n",
    "    - user_id\n",
    "    - transaction_date\n",
    "    - amount\n",
    "    - platform\n",
    "- user_preferences\n",
    "    - user_id\n",
    "    - communication_preference\n",
    "    - interface_theme\n",
    "    - notification_settings\n",
    "      \n",
    "<b>Scenarios:\n",
    "\n",
    "- Active users (logged in last 30 days)\n",
    "- Filter by high-value customers (top 20% by spending)\n",
    "- User preference trends for the identified customers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1d284f-eb9c-4ad1-a72b-f9490782f3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "WITH active_users AS (\n",
    "    SELECT\n",
    "        ua.user_id\n",
    "    FROM\n",
    "        user_activity ua\n",
    "    WHERE\n",
    "        ua.last_login_date >= CURRENT_DATE - INTERVAL '30 days'\n",
    "),\n",
    "\n",
    "user_spending AS (\n",
    "    SELECT\n",
    "        t.user_id,\n",
    "        SUM(t.amount) AS total_spent\n",
    "    FROM\n",
    "        transactions t\n",
    "    WHERE\n",
    "        t.user_id IN (SELECT user_id FROM active_users)\n",
    "    GROUP BY\n",
    "        t.user_id\n",
    "),\n",
    "\n",
    "top_spenders AS (\n",
    "    SELECT\n",
    "        user_id,\n",
    "        total_spent,\n",
    "        NTILE(5) OVER (ORDER BY total_spent DESC) AS spend_quintile\n",
    "    FROM\n",
    "        user_spending\n",
    "),\n",
    "\n",
    "top_20_percent AS (\n",
    "    SELECT\n",
    "        user_id,\n",
    "        total_spent\n",
    "    FROM\n",
    "        top_spenders\n",
    "    WHERE\n",
    "        spend_quintile = 1  -- top 20% (first quintile)\n",
    ")\n",
    "\n",
    "SELECT\n",
    "    ts.user_id,\n",
    "    ts.total_spent,\n",
    "    ua.account_type,\n",
    "    up.communication_preference,\n",
    "    up.interface_theme,\n",
    "    up.notification_settings\n",
    "FROM\n",
    "    top_20_percent ts\n",
    "JOIN\n",
    "    user_activity ua ON ts.user_id = ua.user_id\n",
    "LEFT JOIN\n",
    "    user_preferences up ON ts.user_id = up.user_id\n",
    "ORDER BY\n",
    "    ts.total_spent DESC;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d525cf-5ab8-4a80-b44b-9a2112dddab1",
   "metadata": {},
   "source": [
    "### Discussion Points:\n",
    "\n",
    "1. How were percentile calculations handled?\n",
    "\n",
    "Percentile calculations were handled using the NTILE window function, which divides the ordered dataset into equal-sized groups (quintiles in this case). By ordering users by their total spending in descending order and applying NTILE(5), the top 20% of customers were identified as those in the first quintile. This approach efficiently segments users based on relative spending without requiring complex percentile calculations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce61b28-aaa1-4c46-80ab-dd05fb80443e",
   "metadata": {},
   "source": [
    "2. What approaches to date filtering were used?\n",
    "\n",
    "Date filtering was performed by comparing the last_login_date to the current date minus a 30-day interval (CURRENT_DATE - INTERVAL '30 days'). This approach ensures only users who logged in within the last 30 days are considered active, enabling timely segmentation based on recent activity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce51039-4f75-4990-addf-324cf7123bd1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "3. How was the query optimized, CTE, subquery etc…?\n",
    "\n",
    "The query was optimized using Common Table Expressions (CTEs) to break down complex logic into clear, manageable steps. This modular approach improves readability and maintainability, allows reuse of intermediate results (like active users and spending aggregates), and helps the SQL engine optimize execution by processing each step efficiently before the final join."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
